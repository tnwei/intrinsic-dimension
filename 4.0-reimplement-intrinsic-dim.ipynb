{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61a12d68-0172-44f2-ad87-8f206fc15572",
   "metadata": {},
   "source": [
    "# Reimplementing intrinsic dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa88be4c-df21-406d-bd8e-7df39a245ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "371837f7-472b-41f6-810a-271c03fa4757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b74667-1da4-4766-bc9e-a3d15efb4555",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f5a7f7-4ea5-4fcf-a30b-bb4f615fe936",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(), \n",
    "    torchvision.transforms.Lambda(lambda x: torch.flatten(x))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98c7a272-0e0a-4024-ba2a-2d940322421e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torchvision.datasets.MNIST(\n",
    "    root=\"~/.torchdata/\", download=False, \n",
    "    # natively stored as PIL images\n",
    "    transform=dataset_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca151cfc-b701-484f-9e69-b66f28f7931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torchvision.datasets.MNIST(\n",
    "    root=\"~/.torchdata/\", download=False, \n",
    "    train=False,\n",
    "    transform=dataset_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8cc0e6c-816d-43ab-9ab4-bd206a20babf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: /home/tnwei/.torchdata/\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Lambda()\n",
       "           )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc65c296-3f11-4a29-8abc-19c46f049308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: /home/tnwei/.torchdata/\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Lambda()\n",
       "           )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5f61b17-3163-47ce-aa29-232d1b3184e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73123905-463b-484d-a09c-90d035372ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=100, shuffle=True)\n",
    "# Returns (torch.Size([100, 784]), torch.Size([100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "373185f2-8f5a-4dce-8ca0-17827b813f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec7f45-1680-4795-9e32-9f85695f0260",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reimplementing using only one theta prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19b57c0b-ca37-4eb4-9a84-7820e470b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubspaceLinear(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features, out_features, \n",
    "        theta_prime, \n",
    "        bias: bool = True, # the rest is by the numbers\n",
    "        device = None,\n",
    "        dtype = None\n",
    "    ):\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "\n",
    "        super().__init__() \n",
    "        \n",
    "        # Mirror nn.Linear init\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.subspace_features = theta_prime.shape[0] # (intrinsic_dim, 1)\n",
    "        self.theta_prime = theta_prime\n",
    "        \n",
    "        # Weight has shape (out_features, in_features)\n",
    "        # Therefore P x theta_prime is:\n",
    "        # (out_features, in_features, subspace_features) X (subspace_features, 1)\n",
    "        \n",
    "        # Create and init theta, save theta_zero\n",
    "        self.theta = torch.empty((out_features, in_features), **factory_kwargs)\n",
    "        nn.init.kaiming_uniform_(self.theta, a=math.sqrt(5))\n",
    "        self.theta_zero = self.theta.detach().clone()\n",
    "        \n",
    "        # Generate projection matrix for weights\n",
    "        self.proj_mat_weights = torch.empty((out_features, in_features, self.subspace_features), **factory_kwargs)\n",
    "        nn.init.kaiming_uniform_(self.proj_mat_weights, a=math.sqrt(5))\n",
    "        \n",
    "        if bias:\n",
    "            # Create and init bias, save bias zero\n",
    "            self.bias = torch.empty(out_features, **factory_kwargs)\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.theta_zero)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            nn.init.uniform_(self.bias, -bound, bound)    \n",
    "            self.bias_zero = self.bias.detach().clone()\n",
    "            \n",
    "            # Generate projection matrix for bias\n",
    "            self.proj_mat_bias = torch.empty((out_features, self.subspace_features), **factory_kwargs)\n",
    "            nn.init.kaiming_uniform_(self.proj_mat_bias, a=math.sqrt(5))\n",
    "            \n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in nn.Linear:\n",
    "        # return F.linear(x, self.weight, self.bias)\n",
    "        # torch.mm is for matrices only! torch.matmul is the one that can do broadcasting\n",
    "        theta = self.theta_zero + torch.squeeze(torch.matmul(self.proj_mat_weights, self.theta_prime), dim=-1)\n",
    "        bias = self.bias_zero + torch.squeeze(torch.matmul(self.proj_mat_bias, self.theta_prime), dim=-1)\n",
    "        return F.linear(x, theta, bias)\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}, out_features={}, subspace_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.subspace_features, self.bias is not None\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbb6ba4f-6af0-477a-bfcd-38ffb1256f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsic_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deb5c0ff-5a21-481e-b8db-ddfb1f4aca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_prime = Parameter(torch.empty((intrinsic_dim, 1)))\n",
    "theta_prime.data.fill_(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95b37ebd-e154-4ae7-a91d-db3a2f93f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "sslin1 = SubspaceLinear(in_features=784, out_features=200, theta_prime=theta_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1453bae-a5fe-4aa8-8c1f-87292c4e397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_prime torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, j in sslin1.named_parameters():\n",
    "    print(i, j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b50b3a2-4478-4b63-9b83-971d7272d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sslin2 = SubspaceLinear(in_features=10, out_features=5, theta_prime=theta_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f66ec8d2-8983-429e-903a-5cfc3c205579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_prime torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "for i, j in sslin2.named_parameters():\n",
    "    print(i, j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3da5ac7-e764-4cbc-b464-a95fe91597e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubspaceConstrainedMNIST(nn.Module):\n",
    "    def __init__(self, subspace_features: int, device=\"cpu\"):\n",
    "        \"\"\"\n",
    "        Paper uses 784-200-200-10\n",
    "        ref: https://arxiv.org/pdf/1804.08838.pdf\n",
    "        \n",
    "        Ref in github:\n",
    "        https://github.com/uber-research/intrinsic-dimension/blob/9754ebe1954e82973c7afe280d2c59850f281dca/intrinsic_dim/model_builders.py#L81\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        intrinsic_dim = 10\n",
    "        self.theta_prime = Parameter(torch.empty((intrinsic_dim, 1)))\n",
    "        self.theta_prime.data.fill_(0);\n",
    "    \n",
    "        self.hidden1 = SubspaceLinear(in_features=784, out_features=200, theta_prime=self.theta_prime, device=device)\n",
    "        self.hidden2 = SubspaceLinear(in_features=200, out_features=10, theta_prime=self.theta_prime, device=device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.log_softmax(x, dim=-1)  # (batch_size, dims)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb78eef-2dfa-4322-890f-88762555354c",
   "metadata": {},
   "source": [
    "I think this is it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d0c71-9e47-4bef-9417-cbd338e18461",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9818baf-2889-4605-b4c2-b63aba440a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, num_epochs, train_loader, device=\"cpu\"):\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "    net.train()\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    \n",
    "    for _ in range(num_epochs):\n",
    "        for batch_id, (features, target) in enumerate(train_loader):\n",
    "            # forward pass, calculate loss and backprop!\n",
    "            opt.zero_grad()\n",
    "            preds = net(features.to(device))\n",
    "            loss = F.nll_loss(preds, target.to(device))\n",
    "            loss.backward()\n",
    "            loss_history.append(loss.item())\n",
    "            opt.step()\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                print(loss.item())\n",
    "                \n",
    "    # Verified don't need to return the net\n",
    "    return loss_history, acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4262d4a-cb52-4a23-8b0d-304476154a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(net, test_loader, device=\"cpu\"):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for features, target in test_loader:\n",
    "        output = net(features.to(device))\n",
    "        test_loss += F.nll_loss(output, target.to(device)).item()\n",
    "        pred = torch.argmax(output, dim=-1) # get the index of the max log-probability\n",
    "        correct += pred.eq(target.to(device)).cpu().sum()\n",
    "\n",
    "    test_loss = test_loss\n",
    "    test_loss /= len(test_loader) # loss function already averages over batch size\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    acc_history.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "    \n",
    "    return test_loss, correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfd08e1d-c857-4dae-879a-adf7ead20686",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_histories = {}\n",
    "acc_histories = {}\n",
    "test_losses = {}\n",
    "corrects = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c53297ed-95b5-4eac-b2fe-8602831b2717",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [10, 50, 100, 200, 500, 600, 700, 750, 800, 1000, 1500, 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1a993b7-63de-4969-8abf-7eb0a5b68095",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3053252696990967\n",
      "2.308882474899292\n",
      "2.289276599884033\n",
      "2.298297882080078\n",
      "2.2959940433502197\n",
      "2.2994325160980225\n",
      "2.2925140857696533\n",
      "2.296787977218628\n",
      "2.287163496017456\n",
      "2.2917532920837402\n",
      "2.296682119369507\n",
      "2.280731439590454\n",
      "2.29170823097229\n",
      "2.2875003814697266\n",
      "2.3002285957336426\n",
      "2.2807681560516357\n",
      "2.282684803009033\n",
      "2.290861129760742\n",
      "2.2743921279907227\n",
      "2.2719695568084717\n",
      "2.2855136394500732\n",
      "2.286449909210205\n",
      "2.2929840087890625\n",
      "2.2904856204986572\n",
      "2.2872276306152344\n",
      "2.2857015132904053\n",
      "2.2785494327545166\n",
      "2.3058128356933594\n",
      "2.294235944747925\n",
      "2.2696692943573\n",
      "2.291292190551758\n",
      "2.2872579097747803\n",
      "2.2878148555755615\n",
      "2.277557849884033\n",
      "2.2964565753936768\n",
      "2.2697627544403076\n",
      "2.277078151702881\n",
      "2.2655797004699707\n",
      "2.288762331008911\n",
      "2.278978109359741\n",
      "2.2851271629333496\n",
      "2.281541109085083\n",
      "2.296551465988159\n",
      "2.2647645473480225\n",
      "2.3020975589752197\n",
      "2.2774465084075928\n",
      "2.282496690750122\n",
      "2.308363914489746\n",
      "2.2827672958374023\n",
      "2.270899534225464\n",
      "2.292191505432129\n",
      "2.2825169563293457\n",
      "2.2883548736572266\n",
      "2.2766218185424805\n",
      "2.299992799758911\n",
      "2.2655746936798096\n",
      "2.2825887203216553\n",
      "2.2817251682281494\n",
      "2.2858195304870605\n",
      "2.2883570194244385\n",
      "2.2732205390930176\n",
      "2.2777838706970215\n",
      "2.2654573917388916\n",
      "2.290945053100586\n",
      "2.261270523071289\n",
      "2.2968273162841797\n",
      "2.281172275543213\n",
      "2.2928590774536133\n",
      "2.298765182495117\n",
      "2.2653281688690186\n",
      "2.2667441368103027\n",
      "2.2833995819091797\n",
      "2.2740607261657715\n",
      "2.269622564315796\n",
      "2.300313949584961\n",
      "2.288975238800049\n",
      "2.2773213386535645\n",
      "2.254913091659546\n",
      "2.2817041873931885\n",
      "2.2843024730682373\n",
      "2.314337730407715\n",
      "2.2841010093688965\n",
      "2.27645206451416\n",
      "2.27480149269104\n",
      "2.250201940536499\n",
      "2.2925214767456055\n",
      "2.2858402729034424\n",
      "2.2944257259368896\n",
      "2.2794911861419678\n",
      "2.2796008586883545\n",
      "\n",
      "Test set: Average loss: 2.2746, Accuracy: 1684/10000 (17%)\n",
      "\n",
      "2.3136985301971436\n",
      "2.303488254547119\n",
      "2.298654317855835\n",
      "2.297072649002075\n",
      "2.2948391437530518\n",
      "2.303852081298828\n",
      "2.305130958557129\n",
      "2.2943224906921387\n",
      "2.3039121627807617\n",
      "2.295391798019409\n",
      "2.2991819381713867\n",
      "2.2992095947265625\n",
      "2.296208381652832\n",
      "2.296286106109619\n",
      "2.3032166957855225\n",
      "2.298617124557495\n",
      "2.296316146850586\n",
      "2.2927184104919434\n",
      "2.2803192138671875\n",
      "2.288357734680176\n",
      "2.298433542251587\n",
      "2.2731053829193115\n",
      "2.291334629058838\n",
      "2.291626214981079\n",
      "2.2986350059509277\n",
      "2.313076972961426\n",
      "2.2874228954315186\n",
      "2.2966339588165283\n",
      "2.295980215072632\n",
      "2.313230037689209\n",
      "2.293555974960327\n",
      "2.300464630126953\n",
      "2.303631067276001\n",
      "2.2947466373443604\n",
      "2.2995593547821045\n",
      "2.2958943843841553\n",
      "2.287533760070801\n",
      "2.2976691722869873\n",
      "2.2909860610961914\n",
      "2.2945451736450195\n",
      "2.3015224933624268\n",
      "2.3019590377807617\n",
      "2.2861995697021484\n",
      "2.3057291507720947\n",
      "2.288010597229004\n",
      "2.2989978790283203\n",
      "2.3008241653442383\n",
      "2.307159185409546\n",
      "2.299528121948242\n",
      "2.2914533615112305\n",
      "2.3085079193115234\n",
      "2.299400568008423\n",
      "2.296238899230957\n",
      "2.287292718887329\n",
      "2.2899811267852783\n",
      "2.2999565601348877\n",
      "2.306614875793457\n",
      "2.2773935794830322\n",
      "2.2881460189819336\n",
      "2.2921254634857178\n",
      "2.2976489067077637\n",
      "2.288578987121582\n",
      "2.299978733062744\n",
      "2.2873966693878174\n",
      "2.2953433990478516\n",
      "2.297172784805298\n",
      "2.2918126583099365\n",
      "2.285515785217285\n",
      "2.2917232513427734\n",
      "2.3100905418395996\n",
      "2.3059463500976562\n",
      "2.2935569286346436\n",
      "2.296499252319336\n",
      "2.293164014816284\n",
      "2.291410446166992\n",
      "2.301211357116699\n",
      "2.2987630367279053\n",
      "2.283493757247925\n",
      "2.289877414703369\n",
      "2.292996406555176\n",
      "2.297269344329834\n",
      "2.2832705974578857\n",
      "2.2857556343078613\n",
      "2.289971113204956\n",
      "2.2951319217681885\n",
      "2.2957100868225098\n",
      "2.2855539321899414\n",
      "2.2950146198272705\n",
      "2.291684150695801\n",
      "2.303983449935913\n",
      "\n",
      "Test set: Average loss: 2.2946, Accuracy: 1677/10000 (17%)\n",
      "\n",
      "2.305330276489258\n",
      "2.300685405731201\n",
      "2.298600673675537\n",
      "2.2875585556030273\n",
      "2.2992281913757324\n",
      "2.2929155826568604\n",
      "2.2935006618499756\n",
      "2.30599045753479\n",
      "2.284918785095215\n",
      "2.2902584075927734\n",
      "2.2901923656463623\n",
      "2.30009388923645\n",
      "2.2862443923950195\n",
      "2.288489580154419\n",
      "2.2987258434295654\n",
      "2.2856507301330566\n",
      "2.289285182952881\n",
      "2.289857864379883\n",
      "2.279492139816284\n",
      "2.2662627696990967\n",
      "2.3009557723999023\n",
      "2.283080816268921\n",
      "2.2936980724334717\n",
      "2.2993226051330566\n",
      "2.291259527206421\n",
      "2.2662599086761475\n",
      "2.278552293777466\n",
      "2.2865548133850098\n",
      "2.272200345993042\n",
      "2.2823543548583984\n",
      "2.2872283458709717\n",
      "2.2876737117767334\n",
      "2.2828595638275146\n",
      "2.3025591373443604\n",
      "2.2832062244415283\n",
      "2.266065835952759\n",
      "2.299703359603882\n",
      "2.2909767627716064\n",
      "2.2747914791107178\n",
      "2.2998878955841064\n",
      "2.26792049407959\n",
      "2.292189121246338\n",
      "2.298717498779297\n",
      "2.300814628601074\n",
      "2.275268793106079\n",
      "2.293677568435669\n",
      "2.270123243331909\n",
      "2.2665135860443115\n",
      "2.2631680965423584\n",
      "2.2948451042175293\n",
      "2.2864441871643066\n",
      "2.2854151725769043\n",
      "2.2924535274505615\n",
      "2.2776591777801514\n",
      "2.2591049671173096\n",
      "2.2715132236480713\n",
      "2.2681868076324463\n",
      "2.2933034896850586\n",
      "2.274933099746704\n",
      "2.2487988471984863\n",
      "2.3046810626983643\n",
      "2.2851669788360596\n",
      "2.2899765968322754\n",
      "2.261038303375244\n",
      "2.2891595363616943\n",
      "2.2697677612304688\n",
      "2.2922284603118896\n",
      "2.2643911838531494\n",
      "2.2791919708251953\n",
      "2.2864761352539062\n",
      "2.2437760829925537\n",
      "2.2707581520080566\n",
      "2.2870266437530518\n",
      "2.2901487350463867\n",
      "2.2700467109680176\n",
      "2.2579164505004883\n",
      "2.2684950828552246\n",
      "2.2551097869873047\n",
      "2.258498430252075\n",
      "2.2677814960479736\n",
      "2.260005235671997\n",
      "2.2675483226776123\n",
      "2.266237735748291\n",
      "2.2277779579162598\n",
      "2.2662339210510254\n",
      "2.2970948219299316\n",
      "2.278601884841919\n",
      "2.266913652420044\n",
      "2.244558334350586\n",
      "2.2560503482818604\n",
      "\n",
      "Test set: Average loss: 2.2650, Accuracy: 2417/10000 (24%)\n",
      "\n",
      "2.3032805919647217\n",
      "2.3032279014587402\n",
      "2.296689510345459\n",
      "2.3069887161254883\n",
      "2.2973079681396484\n",
      "2.290310859680176\n",
      "2.296161651611328\n",
      "2.295386552810669\n",
      "2.2970545291900635\n",
      "2.3002872467041016\n",
      "2.29304575920105\n",
      "2.2995331287384033\n",
      "2.3112096786499023\n",
      "2.2886667251586914\n",
      "2.2937445640563965\n",
      "2.288271427154541\n",
      "2.3047845363616943\n",
      "2.2846121788024902\n",
      "2.275087833404541\n",
      "2.2919647693634033\n",
      "2.3032126426696777\n",
      "2.2892050743103027\n",
      "2.3070363998413086\n",
      "2.282665729522705\n",
      "2.2925493717193604\n",
      "2.29653263092041\n",
      "2.275391101837158\n",
      "2.2859625816345215\n",
      "2.286738634109497\n",
      "2.2836461067199707\n",
      "2.2971203327178955\n",
      "2.2871243953704834\n",
      "2.2897849082946777\n",
      "2.2800378799438477\n",
      "2.290024518966675\n",
      "2.2844231128692627\n",
      "2.2982020378112793\n",
      "2.296988010406494\n",
      "2.289478063583374\n",
      "2.2899603843688965\n",
      "2.2861709594726562\n",
      "2.28342342376709\n",
      "2.2719383239746094\n",
      "2.27651309967041\n",
      "2.2892301082611084\n",
      "2.2990562915802\n",
      "2.2885499000549316\n",
      "2.2973825931549072\n",
      "2.2869980335235596\n",
      "2.302483320236206\n",
      "2.270632743835449\n",
      "2.281625986099243\n",
      "2.2887558937072754\n",
      "2.3056650161743164\n",
      "2.2944724559783936\n",
      "2.2915351390838623\n",
      "2.2853691577911377\n",
      "2.2691102027893066\n",
      "2.28178334236145\n",
      "2.301210880279541\n",
      "2.2809484004974365\n",
      "2.2713093757629395\n",
      "2.2856483459472656\n",
      "2.294928789138794\n",
      "2.3073244094848633\n",
      "2.3102850914001465\n",
      "2.2922451496124268\n",
      "2.2958595752716064\n",
      "2.297809362411499\n",
      "2.276984930038452\n",
      "2.276059627532959\n",
      "2.2895357608795166\n",
      "2.2704601287841797\n",
      "2.309386730194092\n",
      "2.2652244567871094\n",
      "2.2719833850860596\n",
      "2.3053789138793945\n",
      "2.2972192764282227\n",
      "2.281756639480591\n",
      "2.297590494155884\n",
      "2.2832252979278564\n",
      "2.296572208404541\n",
      "2.2955260276794434\n",
      "2.2988357543945312\n",
      "2.2788753509521484\n",
      "2.284944534301758\n",
      "2.2757158279418945\n",
      "2.282010078430176\n",
      "2.280325174331665\n",
      "2.2852115631103516\n",
      "\n",
      "Test set: Average loss: 2.2828, Accuracy: 1858/10000 (19%)\n",
      "\n",
      "2.3086068630218506\n",
      "2.302607774734497\n",
      "2.285630941390991\n",
      "2.2835981845855713\n",
      "2.2931509017944336\n",
      "2.311354875564575\n",
      "2.2916667461395264\n",
      "2.305044651031494\n",
      "2.3011341094970703\n",
      "2.2980878353118896\n",
      "2.284461259841919\n",
      "2.2942399978637695\n",
      "2.3006277084350586\n",
      "2.282588243484497\n",
      "2.283846378326416\n",
      "2.309130907058716\n",
      "2.2862181663513184\n",
      "2.297539472579956\n",
      "2.2966558933258057\n",
      "2.295656204223633\n",
      "2.2919394969940186\n",
      "2.3046841621398926\n",
      "2.2801783084869385\n",
      "2.2766289710998535\n",
      "2.291012763977051\n",
      "2.284853458404541\n",
      "2.2921664714813232\n",
      "2.2828433513641357\n",
      "2.2824387550354004\n",
      "2.2941019535064697\n",
      "2.2824621200561523\n",
      "2.284701108932495\n",
      "2.2961630821228027\n",
      "2.2901341915130615\n",
      "2.309117555618286\n",
      "2.2970099449157715\n",
      "2.289059638977051\n",
      "2.288895845413208\n",
      "2.3014326095581055\n",
      "2.307290554046631\n",
      "2.280301094055176\n",
      "2.2919723987579346\n",
      "2.285612106323242\n",
      "2.303326368331909\n",
      "2.3053548336029053\n",
      "2.294065237045288\n",
      "2.305026054382324\n",
      "2.303422689437866\n",
      "2.3029613494873047\n",
      "2.3054494857788086\n",
      "2.296989679336548\n",
      "2.2936623096466064\n",
      "2.3108975887298584\n",
      "2.298100233078003\n",
      "2.284724712371826\n",
      "2.2973527908325195\n",
      "2.2965285778045654\n",
      "2.2792696952819824\n",
      "2.295149803161621\n",
      "2.29551362991333\n",
      "2.2967355251312256\n",
      "2.2971439361572266\n",
      "2.294919729232788\n",
      "2.279602527618408\n",
      "2.289090156555176\n",
      "2.2977347373962402\n",
      "2.2919650077819824\n",
      "2.2976837158203125\n",
      "2.2917532920837402\n",
      "2.289421558380127\n",
      "2.304457187652588\n",
      "2.280083417892456\n",
      "2.303006172180176\n",
      "2.274412155151367\n",
      "2.2890686988830566\n",
      "2.2787675857543945\n",
      "2.2743723392486572\n",
      "2.2759478092193604\n",
      "2.2888312339782715\n",
      "2.2760415077209473\n",
      "2.300724983215332\n",
      "2.2867448329925537\n",
      "2.281714677810669\n",
      "2.2908260822296143\n",
      "2.2939417362213135\n",
      "2.2985432147979736\n",
      "2.2821640968322754\n",
      "2.303842067718506\n",
      "2.2829489707946777\n",
      "2.3013925552368164\n",
      "\n",
      "Test set: Average loss: 2.2843, Accuracy: 1458/10000 (15%)\n",
      "\n",
      "2.2984015941619873\n",
      "2.310824394226074\n",
      "2.307623863220215\n",
      "2.2971372604370117\n",
      "2.2922427654266357\n",
      "2.3011066913604736\n",
      "2.309387445449829\n",
      "2.3044066429138184\n",
      "2.2947614192962646\n",
      "2.298527717590332\n",
      "2.292550802230835\n",
      "2.291905164718628\n",
      "2.3089637756347656\n",
      "2.3036961555480957\n",
      "2.309161424636841\n",
      "2.298128604888916\n",
      "2.290313482284546\n",
      "2.2969229221343994\n",
      "2.279831886291504\n",
      "2.303133487701416\n",
      "2.30710506439209\n",
      "2.296923875808716\n",
      "2.3028464317321777\n",
      "2.2979960441589355\n",
      "2.3031046390533447\n",
      "2.2989354133605957\n",
      "2.291037082672119\n",
      "2.290755271911621\n",
      "2.289689064025879\n",
      "2.2795608043670654\n",
      "2.2942986488342285\n",
      "2.2891428470611572\n",
      "2.2900583744049072\n",
      "2.2920620441436768\n",
      "2.295545816421509\n",
      "2.2951979637145996\n",
      "2.2906055450439453\n",
      "2.2933459281921387\n",
      "2.3027126789093018\n",
      "2.3115737438201904\n",
      "2.2943506240844727\n",
      "2.2956089973449707\n",
      "2.295222520828247\n",
      "2.2836544513702393\n",
      "2.3022239208221436\n",
      "2.3146250247955322\n",
      "2.2916982173919678\n",
      "2.294339179992676\n",
      "2.288191795349121\n",
      "2.2905640602111816\n",
      "2.2791948318481445\n",
      "2.291605234146118\n",
      "2.293008327484131\n",
      "2.2894225120544434\n",
      "2.294172763824463\n",
      "2.2936675548553467\n",
      "2.304680824279785\n",
      "2.2912344932556152\n",
      "2.283743143081665\n",
      "2.2949447631835938\n",
      "2.289581060409546\n",
      "2.3073220252990723\n",
      "2.2878050804138184\n",
      "2.290111541748047\n",
      "2.2910642623901367\n",
      "2.2858057022094727\n",
      "2.2835917472839355\n",
      "2.3003809452056885\n",
      "2.2917327880859375\n",
      "2.2805144786834717\n",
      "2.2848966121673584\n",
      "2.2960851192474365\n",
      "2.294722080230713\n",
      "2.2812750339508057\n",
      "2.2789390087127686\n",
      "2.3132944107055664\n",
      "2.2731611728668213\n",
      "2.301025867462158\n",
      "2.2817044258117676\n",
      "2.287766933441162\n",
      "2.270850419998169\n",
      "2.287824869155884\n",
      "2.2976784706115723\n",
      "2.2859859466552734\n",
      "2.2909319400787354\n",
      "2.259070634841919\n",
      "2.297015905380249\n",
      "2.28108549118042\n",
      "2.269632577896118\n",
      "2.292375326156616\n",
      "\n",
      "Test set: Average loss: 2.2878, Accuracy: 1232/10000 (12%)\n",
      "\n",
      "2.3162169456481934\n",
      "2.2996509075164795\n",
      "2.3052380084991455\n",
      "2.3020780086517334\n",
      "2.285088300704956\n",
      "2.2909767627716064\n",
      "2.290520668029785\n",
      "2.2939629554748535\n",
      "2.298267364501953\n",
      "2.288654088973999\n",
      "2.302039861679077\n",
      "2.300433874130249\n",
      "2.2950632572174072\n",
      "2.2966861724853516\n",
      "2.2971391677856445\n",
      "2.2890284061431885\n",
      "2.2926015853881836\n",
      "2.2931857109069824\n",
      "2.2872910499572754\n",
      "2.2971408367156982\n",
      "2.291625499725342\n",
      "2.283910036087036\n",
      "2.2961151599884033\n",
      "2.2798655033111572\n",
      "2.2853188514709473\n",
      "2.2890572547912598\n",
      "2.3030471801757812\n",
      "2.3051650524139404\n",
      "2.2846601009368896\n",
      "2.292943000793457\n",
      "2.2839510440826416\n",
      "2.2871592044830322\n",
      "2.2859785556793213\n",
      "2.273240566253662\n",
      "2.2936320304870605\n",
      "2.279090642929077\n",
      "2.2913978099823\n",
      "2.2800018787384033\n",
      "2.291440010070801\n",
      "2.304823398590088\n",
      "2.2981698513031006\n",
      "2.27321720123291\n",
      "2.2969248294830322\n",
      "2.29377818107605\n",
      "2.2788002490997314\n",
      "2.3012540340423584\n",
      "2.286451816558838\n",
      "2.2712841033935547\n",
      "2.292830228805542\n",
      "2.2916643619537354\n",
      "2.273315191268921\n",
      "2.2799618244171143\n",
      "2.259166955947876\n",
      "2.274226665496826\n",
      "2.2617294788360596\n",
      "2.255764961242676\n",
      "2.2897722721099854\n",
      "2.290828227996826\n",
      "2.290803909301758\n",
      "2.268638849258423\n",
      "2.2838802337646484\n",
      "2.279987335205078\n",
      "2.2496581077575684\n",
      "2.245342493057251\n",
      "2.258260726928711\n",
      "2.2494115829467773\n",
      "2.289844036102295\n",
      "2.2817444801330566\n",
      "2.2745885848999023\n",
      "2.2611467838287354\n",
      "2.263507604598999\n",
      "2.2845957279205322\n",
      "2.2727692127227783\n",
      "2.2613236904144287\n",
      "2.3021938800811768\n",
      "2.2887728214263916\n",
      "2.2649195194244385\n",
      "2.266120672225952\n",
      "2.2871716022491455\n",
      "2.2379705905914307\n",
      "2.2563111782073975\n",
      "2.2405006885528564\n",
      "2.2765183448791504\n",
      "2.2690327167510986\n",
      "2.2437872886657715\n",
      "2.237680435180664\n",
      "2.2573022842407227\n",
      "2.2750861644744873\n",
      "2.278892755508423\n",
      "2.301640510559082\n",
      "\n",
      "Test set: Average loss: 2.2617, Accuracy: 1782/10000 (18%)\n",
      "\n",
      "2.2930688858032227\n",
      "2.2942943572998047\n",
      "2.2940638065338135\n",
      "2.2975356578826904\n",
      "2.2935268878936768\n",
      "2.2937793731689453\n",
      "2.29502010345459\n",
      "2.288182258605957\n",
      "2.2832727432250977\n",
      "2.2764792442321777\n",
      "2.2861711978912354\n",
      "2.2904300689697266\n",
      "2.282137870788574\n",
      "2.2690398693084717\n",
      "2.2621726989746094\n",
      "2.2908880710601807\n",
      "2.2944247722625732\n",
      "2.2845261096954346\n",
      "2.278071403503418\n",
      "2.2708873748779297\n",
      "2.2731590270996094\n",
      "2.280890941619873\n",
      "2.2747035026550293\n",
      "2.2946624755859375\n",
      "2.2869760990142822\n",
      "2.2958149909973145\n",
      "2.2922019958496094\n",
      "2.2843942642211914\n",
      "2.311325788497925\n",
      "2.273747205734253\n",
      "2.273198127746582\n",
      "2.283130645751953\n",
      "2.2828311920166016\n",
      "2.272073268890381\n",
      "2.264564275741577\n",
      "2.2954883575439453\n",
      "2.2848615646362305\n",
      "2.283450126647949\n",
      "2.242799758911133\n",
      "2.2867887020111084\n",
      "2.2639529705047607\n",
      "2.2713537216186523\n",
      "2.303861379623413\n",
      "2.3026368618011475\n",
      "2.281060218811035\n",
      "2.276771068572998\n",
      "2.2830886840820312\n",
      "2.2907822132110596\n",
      "2.2650623321533203\n",
      "2.2545130252838135\n",
      "2.2845618724823\n",
      "2.284371852874756\n",
      "2.3089349269866943\n",
      "2.2610092163085938\n",
      "2.27131724357605\n",
      "2.2951996326446533\n",
      "2.27898907661438\n",
      "2.2709662914276123\n",
      "2.295558452606201\n",
      "2.300553798675537\n",
      "2.29232120513916\n",
      "2.2728641033172607\n",
      "2.2673985958099365\n",
      "2.2841100692749023\n",
      "2.300257682800293\n",
      "2.299349546432495\n",
      "2.2875640392303467\n",
      "2.2843894958496094\n",
      "2.294700860977173\n",
      "2.299351215362549\n",
      "2.296783924102783\n",
      "2.2676291465759277\n",
      "2.2933363914489746\n",
      "2.2745542526245117\n",
      "2.2812273502349854\n",
      "2.29209303855896\n",
      "2.295711040496826\n",
      "2.2854835987091064\n",
      "2.2728934288024902\n",
      "2.2858667373657227\n",
      "2.292130470275879\n",
      "2.2782301902770996\n",
      "2.28439998626709\n",
      "2.2978599071502686\n",
      "2.285336971282959\n",
      "2.250187873840332\n",
      "2.296196937561035\n",
      "2.277033805847168\n",
      "2.281599760055542\n",
      "2.2749102115631104\n",
      "\n",
      "Test set: Average loss: 2.2791, Accuracy: 1546/10000 (15%)\n",
      "\n",
      "2.2940142154693604\n",
      "2.291710376739502\n",
      "2.2817904949188232\n",
      "2.2861359119415283\n",
      "2.2929391860961914\n",
      "2.287128448486328\n",
      "2.2878618240356445\n",
      "2.286569595336914\n",
      "2.2906501293182373\n",
      "2.290374517440796\n",
      "2.293429136276245\n",
      "2.2915117740631104\n",
      "2.289701223373413\n",
      "2.2828633785247803\n",
      "2.2804856300354004\n",
      "2.2725067138671875\n",
      "2.2861597537994385\n",
      "2.295672655105591\n",
      "2.2902305126190186\n",
      "2.2804453372955322\n",
      "2.2886528968811035\n",
      "2.2923667430877686\n",
      "2.283139944076538\n",
      "2.2930257320404053\n",
      "2.2895119190216064\n",
      "2.2830917835235596\n",
      "2.2938456535339355\n",
      "2.2951912879943848\n",
      "2.285757064819336\n",
      "2.288893222808838\n",
      "2.2992873191833496\n",
      "2.2822694778442383\n",
      "2.29005765914917\n",
      "2.2853052616119385\n",
      "2.287278175354004\n",
      "2.2818901538848877\n",
      "2.280752658843994\n",
      "2.2849509716033936\n",
      "2.2885353565216064\n",
      "2.2860453128814697\n",
      "2.2850658893585205\n",
      "2.2800073623657227\n",
      "2.2824153900146484\n",
      "2.2881033420562744\n",
      "2.3010971546173096\n",
      "2.293590545654297\n",
      "2.293389081954956\n",
      "2.2749974727630615\n",
      "2.2807459831237793\n",
      "2.2910077571868896\n",
      "2.2835755348205566\n",
      "2.2751901149749756\n",
      "2.2704029083251953\n",
      "2.276648998260498\n",
      "2.275940179824829\n",
      "2.2837371826171875\n",
      "2.2781784534454346\n",
      "2.2809324264526367\n",
      "2.264204263687134\n",
      "2.26627254486084\n",
      "2.271207094192505\n",
      "2.264281988143921\n",
      "2.2800748348236084\n",
      "2.2603816986083984\n",
      "2.2687275409698486\n",
      "2.2684214115142822\n",
      "2.2644455432891846\n",
      "2.261155843734741\n",
      "2.270007610321045\n",
      "2.2431910037994385\n",
      "2.255878448486328\n",
      "2.255270481109619\n",
      "2.242121458053589\n",
      "2.247797727584839\n",
      "2.2331976890563965\n",
      "2.2131383419036865\n",
      "2.254079818725586\n",
      "2.2714426517486572\n",
      "2.2499351501464844\n",
      "2.2406065464019775\n",
      "2.22300386428833\n",
      "2.2261977195739746\n",
      "2.210343599319458\n",
      "2.2289392948150635\n",
      "2.210205078125\n",
      "2.2170233726501465\n",
      "2.209409713745117\n",
      "2.22284197807312\n",
      "2.25494647026062\n",
      "2.233027458190918\n",
      "\n",
      "Test set: Average loss: 2.2205, Accuracy: 2479/10000 (25%)\n",
      "\n",
      "2.3067455291748047\n",
      "2.302271842956543\n",
      "2.302884578704834\n",
      "2.294438362121582\n",
      "2.2920141220092773\n",
      "2.3038103580474854\n",
      "2.2907559871673584\n",
      "2.2830817699432373\n",
      "2.2902777194976807\n",
      "2.290497064590454\n",
      "2.3035073280334473\n",
      "2.3042030334472656\n",
      "2.3080217838287354\n",
      "2.292302131652832\n",
      "2.308112144470215\n",
      "2.2896206378936768\n",
      "2.289634943008423\n",
      "2.2981481552124023\n",
      "2.2932510375976562\n",
      "2.2831692695617676\n",
      "2.273024082183838\n",
      "2.293379306793213\n",
      "2.2864937782287598\n",
      "2.304509162902832\n",
      "2.3033111095428467\n",
      "2.30497407913208\n",
      "2.299166440963745\n",
      "2.2868492603302\n",
      "2.2939181327819824\n",
      "2.2914462089538574\n",
      "2.2953155040740967\n",
      "2.283648729324341\n",
      "2.3007636070251465\n",
      "2.290133237838745\n",
      "2.2697858810424805\n",
      "2.2769317626953125\n",
      "2.3107666969299316\n",
      "2.301694393157959\n",
      "2.2991411685943604\n",
      "2.281977891921997\n",
      "2.278488874435425\n",
      "2.285865545272827\n",
      "2.3160080909729004\n",
      "2.3061530590057373\n",
      "2.3072071075439453\n",
      "2.2851319313049316\n",
      "2.298894166946411\n",
      "2.298032283782959\n",
      "2.2771527767181396\n",
      "2.2707648277282715\n",
      "2.2935914993286133\n",
      "2.273761749267578\n",
      "2.2785654067993164\n",
      "2.2928781509399414\n",
      "2.2842109203338623\n",
      "2.2878329753875732\n",
      "2.2876577377319336\n",
      "2.2833003997802734\n",
      "2.287358283996582\n",
      "2.279852867126465\n",
      "2.272305488586426\n",
      "2.301301956176758\n",
      "2.266205310821533\n",
      "2.2707056999206543\n",
      "2.2991528511047363\n",
      "2.311511516571045\n",
      "2.2749767303466797\n",
      "2.2660534381866455\n",
      "2.2689051628112793\n",
      "2.265596628189087\n",
      "2.295374870300293\n",
      "2.27766489982605\n",
      "2.3022689819335938\n",
      "2.2653989791870117\n",
      "2.2781620025634766\n",
      "2.3102478981018066\n",
      "2.3091719150543213\n",
      "2.2494635581970215\n",
      "2.3102359771728516\n",
      "2.279432773590088\n",
      "2.2689151763916016\n",
      "2.274566888809204\n",
      "2.263774871826172\n",
      "2.258206605911255\n",
      "2.2380530834198\n",
      "2.260563850402832\n",
      "2.2779288291931152\n",
      "2.302818536758423\n",
      "2.2773139476776123\n",
      "2.2611279487609863\n",
      "\n",
      "Test set: Average loss: 2.2700, Accuracy: 2096/10000 (21%)\n",
      "\n",
      "2.307185411453247\n",
      "2.3125803470611572\n",
      "2.301421880722046\n",
      "2.302374839782715\n",
      "2.308209180831909\n",
      "2.298107385635376\n",
      "2.2968263626098633\n",
      "2.2880849838256836\n",
      "2.297696590423584\n",
      "2.295473575592041\n",
      "2.296698808670044\n",
      "2.2967450618743896\n",
      "2.2926836013793945\n",
      "2.302365779876709\n",
      "2.2917118072509766\n",
      "2.304922342300415\n",
      "2.29118275642395\n",
      "2.2886340618133545\n",
      "2.296588897705078\n",
      "2.296548843383789\n",
      "2.292654275894165\n",
      "2.299959182739258\n",
      "2.292118549346924\n",
      "2.299927234649658\n",
      "2.288071393966675\n",
      "2.299677848815918\n",
      "2.299302816390991\n",
      "2.3032119274139404\n",
      "2.2935853004455566\n",
      "2.288515567779541\n",
      "2.3005712032318115\n",
      "2.2962217330932617\n",
      "2.294532060623169\n",
      "2.2958321571350098\n",
      "2.291767120361328\n",
      "2.3042261600494385\n",
      "2.3109941482543945\n",
      "2.2896888256073\n",
      "2.2961554527282715\n",
      "2.2965526580810547\n",
      "2.290562629699707\n",
      "2.2914774417877197\n",
      "2.3037033081054688\n",
      "2.298760175704956\n",
      "2.289726972579956\n",
      "2.291159152984619\n",
      "2.2973532676696777\n",
      "2.3034002780914307\n",
      "2.300041437149048\n",
      "2.281507968902588\n",
      "2.2982027530670166\n",
      "2.291299819946289\n",
      "2.2886462211608887\n",
      "2.2968552112579346\n",
      "2.2987918853759766\n",
      "2.2971580028533936\n",
      "2.3051347732543945\n",
      "2.2952871322631836\n",
      "2.290910005569458\n",
      "2.2913107872009277\n",
      "2.2949821949005127\n",
      "2.295053005218506\n",
      "2.2850844860076904\n",
      "2.280606269836426\n",
      "2.297731637954712\n",
      "2.2804553508758545\n",
      "2.292585849761963\n",
      "2.2892251014709473\n",
      "2.296691417694092\n",
      "2.2928261756896973\n",
      "2.2952957153320312\n",
      "2.286588191986084\n",
      "2.285966396331787\n",
      "2.2954490184783936\n",
      "2.2935984134674072\n",
      "2.279340982437134\n",
      "2.286802291870117\n",
      "2.282977819442749\n",
      "2.298842430114746\n",
      "2.2878339290618896\n",
      "2.2924301624298096\n",
      "2.2807326316833496\n",
      "2.2849440574645996\n",
      "2.296522855758667\n",
      "2.300055503845215\n",
      "2.2868528366088867\n",
      "2.2985575199127197\n",
      "2.287158727645874\n",
      "2.2865991592407227\n",
      "2.2991340160369873\n",
      "\n",
      "Test set: Average loss: 2.2887, Accuracy: 1637/10000 (16%)\n",
      "\n",
      "2.3040192127227783\n",
      "2.2988169193267822\n",
      "2.299708604812622\n",
      "2.297853946685791\n",
      "2.2964775562286377\n",
      "2.2898125648498535\n",
      "2.2927610874176025\n",
      "2.2857067584991455\n",
      "2.293543815612793\n",
      "2.2948687076568604\n",
      "2.2918028831481934\n",
      "2.2998321056365967\n",
      "2.292713165283203\n",
      "2.3074758052825928\n",
      "2.2971014976501465\n",
      "2.2862117290496826\n",
      "2.299901008605957\n",
      "2.2671926021575928\n",
      "2.28104829788208\n",
      "2.288699150085449\n",
      "2.280202865600586\n",
      "2.2805991172790527\n",
      "2.2754340171813965\n",
      "2.291506290435791\n",
      "2.278663396835327\n",
      "2.283179998397827\n",
      "2.2960474491119385\n",
      "2.2820394039154053\n",
      "2.287642002105713\n",
      "2.2978289127349854\n",
      "2.284303665161133\n",
      "2.27199649810791\n",
      "2.2837603092193604\n",
      "2.2760708332061768\n",
      "2.2621376514434814\n",
      "2.304626703262329\n",
      "2.29380202293396\n",
      "2.2808678150177\n",
      "2.2894394397735596\n",
      "2.2856221199035645\n",
      "2.2615983486175537\n",
      "2.2881128787994385\n",
      "2.2806618213653564\n",
      "2.305241823196411\n",
      "2.2842061519622803\n",
      "2.2627265453338623\n",
      "2.26540470123291\n",
      "2.2868287563323975\n",
      "2.270077705383301\n",
      "2.2453770637512207\n",
      "2.249922752380371\n",
      "2.259035587310791\n",
      "2.2816317081451416\n",
      "2.288987636566162\n",
      "2.2792582511901855\n",
      "2.269848585128784\n",
      "2.2916312217712402\n",
      "2.2761287689208984\n",
      "2.2963268756866455\n",
      "2.276458263397217\n",
      "2.2894082069396973\n",
      "2.259084939956665\n",
      "2.2693114280700684\n",
      "2.261843204498291\n",
      "2.244058609008789\n",
      "2.273346185684204\n",
      "2.2578887939453125\n",
      "2.290330648422241\n",
      "2.2717125415802\n",
      "2.2663538455963135\n",
      "2.253754138946533\n",
      "2.2692573070526123\n",
      "2.2625348567962646\n",
      "2.288907527923584\n",
      "2.256098747253418\n",
      "2.259636878967285\n",
      "2.265409231185913\n",
      "2.2394022941589355\n",
      "2.247440814971924\n",
      "2.2706422805786133\n",
      "2.279360055923462\n",
      "2.3026175498962402\n",
      "2.27748441696167\n",
      "2.2824759483337402\n",
      "2.2976326942443848\n",
      "2.293269157409668\n",
      "2.2638776302337646\n",
      "2.2727324962615967\n",
      "2.3086376190185547\n",
      "2.272545576095581\n",
      "\n",
      "Test set: Average loss: 2.2726, Accuracy: 1826/10000 (18%)\n",
      "\n",
      "2.301197052001953\n",
      "2.310692071914673\n",
      "2.3014142513275146\n",
      "2.3063886165618896\n",
      "2.296950101852417\n",
      "2.3010077476501465\n",
      "2.3115174770355225\n",
      "2.2985947132110596\n",
      "2.2921502590179443\n",
      "2.2929694652557373\n",
      "2.275710105895996\n",
      "2.3014984130859375\n",
      "2.288330316543579\n",
      "2.289527654647827\n",
      "2.2827560901641846\n",
      "2.3063626289367676\n",
      "2.303224802017212\n",
      "2.291691303253174\n",
      "2.2774312496185303\n",
      "2.29763126373291\n",
      "2.282907009124756\n",
      "2.2962095737457275\n",
      "2.2974393367767334\n",
      "2.3028643131256104\n",
      "2.2903473377227783\n",
      "2.312987804412842\n",
      "2.2887845039367676\n",
      "2.283167839050293\n",
      "2.2877445220947266\n",
      "2.2944483757019043\n",
      "2.309907913208008\n",
      "2.3058300018310547\n",
      "2.2796685695648193\n",
      "2.287890672683716\n",
      "2.29972505569458\n",
      "2.296954870223999\n",
      "2.285820245742798\n",
      "2.295098066329956\n",
      "2.286202907562256\n",
      "2.3069915771484375\n",
      "2.279017210006714\n",
      "2.3077754974365234\n",
      "2.2773561477661133\n",
      "2.310859441757202\n",
      "2.285886287689209\n",
      "2.3014397621154785\n",
      "2.3069510459899902\n",
      "2.2947685718536377\n",
      "2.3051345348358154\n",
      "2.3000094890594482\n",
      "2.2771317958831787\n",
      "2.290337324142456\n",
      "2.308802843093872\n",
      "2.313913345336914\n",
      "2.287916421890259\n",
      "2.3064939975738525\n",
      "2.3058197498321533\n",
      "2.2866578102111816\n",
      "2.29219913482666\n",
      "2.298942804336548\n",
      "2.278191089630127\n",
      "2.2992472648620605\n",
      "2.3009438514709473\n",
      "2.282315254211426\n",
      "2.297865390777588\n",
      "2.2912027835845947\n",
      "2.2957096099853516\n",
      "2.2942452430725098\n",
      "2.314291000366211\n",
      "2.2952873706817627\n",
      "2.289877414703369\n",
      "2.3073782920837402\n",
      "2.288513660430908\n",
      "2.294816017150879\n",
      "2.2942216396331787\n",
      "2.295499563217163\n",
      "2.2888176441192627\n",
      "2.30928111076355\n",
      "2.2922873497009277\n",
      "2.295811414718628\n",
      "2.291001796722412\n",
      "2.297396659851074\n",
      "2.3017406463623047\n",
      "2.295186996459961\n",
      "2.3076364994049072\n",
      "2.2912814617156982\n",
      "2.301121711730957\n",
      "2.303025007247925\n",
      "2.274554491043091\n",
      "2.288308620452881\n",
      "\n",
      "Test set: Average loss: 2.2952, Accuracy: 1462/10000 (15%)\n",
      "\n",
      "2.3105154037475586\n",
      "2.3066790103912354\n",
      "2.303961992263794\n",
      "2.294236660003662\n",
      "2.300884962081909\n",
      "2.302395820617676\n",
      "2.3008546829223633\n",
      "2.2966556549072266\n",
      "2.306541681289673\n",
      "2.3071417808532715\n",
      "2.302119493484497\n",
      "2.299055337905884\n",
      "2.310342788696289\n",
      "2.3000879287719727\n",
      "2.3015201091766357\n",
      "2.298349618911743\n",
      "2.297391653060913\n",
      "2.3000621795654297\n",
      "2.2997019290924072\n",
      "2.3037757873535156\n",
      "2.298466444015503\n",
      "2.300503730773926\n",
      "2.299429178237915\n",
      "2.3008954524993896\n",
      "2.300548553466797\n",
      "2.2910666465759277\n",
      "2.302886724472046\n",
      "2.2990317344665527\n",
      "2.2948110103607178\n",
      "2.293017625808716\n",
      "2.301339626312256\n",
      "2.2902987003326416\n",
      "2.2900800704956055\n",
      "2.3094208240509033\n",
      "2.2959372997283936\n",
      "2.2882280349731445\n",
      "2.3012900352478027\n",
      "2.2898409366607666\n",
      "2.281972885131836\n",
      "2.286832809448242\n",
      "2.3107168674468994\n",
      "2.261768102645874\n",
      "2.3015670776367188\n",
      "2.2841784954071045\n",
      "2.292985200881958\n",
      "2.3045454025268555\n",
      "2.2723281383514404\n",
      "2.2968478202819824\n",
      "2.2968733310699463\n",
      "2.270251750946045\n",
      "2.284334182739258\n",
      "2.2866709232330322\n",
      "2.2928779125213623\n",
      "2.309659004211426\n",
      "2.306305170059204\n",
      "2.2902181148529053\n",
      "2.283470869064331\n",
      "2.2874603271484375\n",
      "2.2864086627960205\n",
      "2.275362253189087\n",
      "2.28539776802063\n",
      "2.2963693141937256\n",
      "2.285978078842163\n",
      "2.278494119644165\n",
      "2.275966167449951\n",
      "2.270095109939575\n",
      "2.2943944931030273\n",
      "2.290658950805664\n",
      "2.2793571949005127\n",
      "2.278132438659668\n",
      "2.2831592559814453\n",
      "2.2946386337280273\n",
      "2.302614450454712\n",
      "2.2781169414520264\n",
      "2.295536994934082\n",
      "2.288874626159668\n",
      "2.2869579792022705\n",
      "2.291905164718628\n",
      "2.2859344482421875\n",
      "2.285855770111084\n",
      "2.302316665649414\n",
      "2.292280912399292\n",
      "2.2928569316864014\n",
      "2.289411783218384\n",
      "2.290008783340454\n",
      "2.2960758209228516\n",
      "2.2833478450775146\n",
      "2.277578830718994\n",
      "2.2778491973876953\n",
      "2.2902958393096924\n",
      "\n",
      "Test set: Average loss: 2.2823, Accuracy: 1896/10000 (19%)\n",
      "\n",
      "2.309065103530884\n",
      "2.3137569427490234\n",
      "2.306442975997925\n",
      "2.3056304454803467\n",
      "2.2911319732666016\n",
      "2.2988529205322266\n",
      "2.3049657344818115\n",
      "2.3017990589141846\n",
      "2.3052268028259277\n",
      "2.300049066543579\n",
      "2.2960007190704346\n",
      "2.309689521789551\n",
      "2.2971432209014893\n",
      "2.304971218109131\n",
      "2.3025190830230713\n",
      "2.2956106662750244\n",
      "2.2830560207366943\n",
      "2.3030731678009033\n",
      "2.3001787662506104\n",
      "2.300353527069092\n",
      "2.2871854305267334\n",
      "2.2961766719818115\n",
      "2.2769947052001953\n",
      "2.293747663497925\n",
      "2.29923939704895\n",
      "2.2961935997009277\n",
      "2.306051731109619\n",
      "2.28842830657959\n",
      "2.3041772842407227\n",
      "2.2963762283325195\n",
      "2.2938027381896973\n",
      "2.2942380905151367\n",
      "2.2972681522369385\n",
      "2.3019421100616455\n",
      "2.288149833679199\n",
      "2.295001268386841\n",
      "2.293421745300293\n",
      "2.2944273948669434\n",
      "2.2927401065826416\n",
      "2.283828020095825\n",
      "2.298900842666626\n",
      "2.2869272232055664\n",
      "2.2888104915618896\n",
      "2.3015177249908447\n",
      "2.2961175441741943\n",
      "2.2873787879943848\n",
      "2.2888169288635254\n",
      "2.2932724952697754\n",
      "2.286858320236206\n",
      "2.2916996479034424\n",
      "2.2805285453796387\n",
      "2.278897285461426\n",
      "2.282736301422119\n",
      "2.2963390350341797\n",
      "2.2949843406677246\n",
      "2.2979788780212402\n",
      "2.2869417667388916\n",
      "2.285393238067627\n",
      "2.2895803451538086\n",
      "2.3022260665893555\n",
      "2.2723491191864014\n",
      "2.270751714706421\n",
      "2.281122922897339\n",
      "2.2918951511383057\n",
      "2.2906861305236816\n",
      "2.2729580402374268\n",
      "2.278456687927246\n",
      "2.2692434787750244\n",
      "2.2928197383880615\n",
      "2.275300979614258\n",
      "2.2905025482177734\n",
      "2.2764692306518555\n",
      "2.2641260623931885\n",
      "2.2754998207092285\n",
      "2.2885425090789795\n",
      "2.259462833404541\n",
      "2.287492275238037\n",
      "2.2707855701446533\n",
      "2.269706964492798\n",
      "2.2750871181488037\n",
      "2.277367353439331\n",
      "2.2887368202209473\n",
      "2.2778167724609375\n",
      "2.2732198238372803\n",
      "2.2734789848327637\n",
      "2.280137300491333\n",
      "2.252929925918579\n",
      "2.2888948917388916\n",
      "2.2795958518981934\n",
      "2.267049551010132\n",
      "\n",
      "Test set: Average loss: 2.2733, Accuracy: 1837/10000 (18%)\n",
      "\n",
      "2.30324387550354\n",
      "2.297832489013672\n",
      "2.3072738647460938\n",
      "2.3042728900909424\n",
      "2.3110923767089844\n",
      "2.304875373840332\n",
      "2.2944366931915283\n",
      "2.3008832931518555\n",
      "2.2909843921661377\n",
      "2.2876996994018555\n",
      "2.298337459564209\n",
      "2.311683177947998\n",
      "2.302032232284546\n",
      "2.2927942276000977\n",
      "2.3144190311431885\n",
      "2.283688545227051\n",
      "2.310527801513672\n",
      "2.3009281158447266\n",
      "2.2859573364257812\n",
      "2.290405511856079\n",
      "2.2893970012664795\n",
      "2.3020529747009277\n",
      "2.298846483230591\n",
      "2.294888973236084\n",
      "2.307257890701294\n",
      "2.294912338256836\n",
      "2.3088881969451904\n",
      "2.2896318435668945\n",
      "2.2964959144592285\n",
      "2.269843578338623\n",
      "2.283360719680786\n",
      "2.2958686351776123\n",
      "2.292508840560913\n",
      "2.2734813690185547\n",
      "2.2837092876434326\n",
      "2.2627620697021484\n",
      "2.298767328262329\n",
      "2.289217948913574\n",
      "2.2835419178009033\n",
      "2.274589776992798\n",
      "2.3198347091674805\n",
      "2.2915167808532715\n",
      "2.283950090408325\n",
      "2.278578758239746\n",
      "2.2662625312805176\n",
      "2.3033316135406494\n",
      "2.2773630619049072\n",
      "2.2834057807922363\n",
      "2.279244899749756\n",
      "2.307652235031128\n",
      "2.2758398056030273\n",
      "2.280928134918213\n",
      "2.286496639251709\n",
      "2.295893907546997\n",
      "2.27095627784729\n",
      "2.2441914081573486\n",
      "2.2796099185943604\n",
      "2.2908692359924316\n",
      "2.302366256713867\n",
      "2.2762529850006104\n",
      "2.285759925842285\n",
      "2.269896984100342\n",
      "2.2863924503326416\n",
      "2.2859206199645996\n",
      "2.283579111099243\n",
      "2.283012866973877\n",
      "2.2797205448150635\n",
      "2.2720537185668945\n",
      "2.27896785736084\n",
      "2.278550863265991\n",
      "2.26863169670105\n",
      "2.2750051021575928\n",
      "2.268540859222412\n",
      "2.255794048309326\n",
      "2.2857723236083984\n",
      "2.2905163764953613\n",
      "2.2815968990325928\n",
      "2.2830328941345215\n",
      "2.3009023666381836\n",
      "2.2629826068878174\n",
      "2.258573055267334\n",
      "2.2777276039123535\n",
      "2.279646873474121\n",
      "2.270782232284546\n",
      "2.2208242416381836\n",
      "2.282782554626465\n",
      "2.2703359127044678\n",
      "2.2920360565185547\n",
      "2.2576067447662354\n",
      "2.2776966094970703\n",
      "\n",
      "Test set: Average loss: 2.2678, Accuracy: 1863/10000 (19%)\n",
      "\n",
      "2.308677911758423\n",
      "2.305610418319702\n",
      "2.302689552307129\n",
      "2.3052191734313965\n",
      "2.299097776412964\n",
      "2.2984132766723633\n",
      "2.299781322479248\n",
      "2.303020715713501\n",
      "2.304020643234253\n",
      "2.2886624336242676\n",
      "2.3012382984161377\n",
      "2.3003790378570557\n",
      "2.3006632328033447\n",
      "2.2997732162475586\n",
      "2.2841546535491943\n",
      "2.3142261505126953\n",
      "2.282555341720581\n",
      "2.296475887298584\n",
      "2.2872090339660645\n",
      "2.281771183013916\n",
      "2.2832694053649902\n",
      "2.280613899230957\n",
      "2.2961037158966064\n",
      "2.2905666828155518\n",
      "2.2952303886413574\n",
      "2.2885923385620117\n",
      "2.2833313941955566\n",
      "2.3028717041015625\n",
      "2.2664427757263184\n",
      "2.305396795272827\n",
      "2.2953145503997803\n",
      "2.293879747390747\n",
      "2.291452646255493\n",
      "2.3115041255950928\n",
      "2.3027820587158203\n",
      "2.317732572555542\n",
      "2.3034145832061768\n",
      "2.2941982746124268\n",
      "2.2721049785614014\n",
      "2.2982337474823\n",
      "2.2932097911834717\n",
      "2.3098626136779785\n",
      "2.2749712467193604\n",
      "2.2950124740600586\n",
      "2.295048952102661\n",
      "2.2924704551696777\n",
      "2.2955288887023926\n",
      "2.291851282119751\n",
      "2.3054285049438477\n",
      "2.2855067253112793\n",
      "2.2840094566345215\n",
      "2.2852039337158203\n",
      "2.291701555252075\n",
      "2.2946791648864746\n",
      "2.290432929992676\n",
      "2.2877790927886963\n",
      "2.3166921138763428\n",
      "2.305079698562622\n",
      "2.2871718406677246\n",
      "2.2987706661224365\n",
      "2.279146194458008\n",
      "2.307439088821411\n",
      "2.291673421859741\n",
      "2.2843618392944336\n",
      "2.286278247833252\n",
      "2.306610584259033\n",
      "2.286437749862671\n",
      "2.283841371536255\n",
      "2.292070150375366\n",
      "2.304150104522705\n",
      "2.290477991104126\n",
      "2.303523540496826\n",
      "2.294361114501953\n",
      "2.297400712966919\n",
      "2.275691032409668\n",
      "2.3094322681427\n",
      "2.289098024368286\n",
      "2.285005569458008\n",
      "2.3005805015563965\n",
      "2.3005025386810303\n",
      "2.301750659942627\n",
      "2.2808196544647217\n",
      "2.2978360652923584\n",
      "2.2934606075286865\n",
      "2.2972922325134277\n",
      "2.288909912109375\n",
      "2.2968432903289795\n",
      "2.289825201034546\n",
      "2.2887237071990967\n",
      "2.2941370010375977\n",
      "\n",
      "Test set: Average loss: 2.2887, Accuracy: 1481/10000 (15%)\n",
      "\n",
      "2.308051824569702\n",
      "2.2933878898620605\n",
      "2.3046627044677734\n",
      "2.2982959747314453\n",
      "2.2982888221740723\n",
      "2.305603265762329\n",
      "2.2946059703826904\n",
      "2.304204225540161\n",
      "2.298825979232788\n",
      "2.3039963245391846\n",
      "2.3043863773345947\n",
      "2.2879843711853027\n",
      "2.296590805053711\n",
      "2.302304744720459\n",
      "2.292846918106079\n",
      "2.307175397872925\n",
      "2.2807576656341553\n",
      "2.298844575881958\n",
      "2.296830415725708\n",
      "2.2963638305664062\n",
      "2.2886338233947754\n",
      "2.2959039211273193\n",
      "2.3001816272735596\n",
      "2.2929184436798096\n",
      "2.2892589569091797\n",
      "2.308840751647949\n",
      "2.2886884212493896\n",
      "2.295687198638916\n",
      "2.2884087562561035\n",
      "2.296828269958496\n",
      "2.2936203479766846\n",
      "2.3004772663116455\n",
      "2.2982778549194336\n",
      "2.2951622009277344\n",
      "2.2963578701019287\n",
      "2.2957699298858643\n",
      "2.282193422317505\n",
      "2.2861146926879883\n",
      "2.2942209243774414\n",
      "2.2935943603515625\n",
      "2.290778398513794\n",
      "2.281207323074341\n",
      "2.29532527923584\n",
      "2.2986245155334473\n",
      "2.2841298580169678\n",
      "2.286306142807007\n",
      "2.2814743518829346\n",
      "2.296538829803467\n",
      "2.295714855194092\n",
      "2.3028571605682373\n",
      "2.2962753772735596\n",
      "2.2926077842712402\n",
      "2.289654493331909\n",
      "2.2922301292419434\n",
      "2.284989833831787\n",
      "2.2933967113494873\n",
      "2.2998428344726562\n",
      "2.2984540462493896\n",
      "2.296225070953369\n",
      "2.2832677364349365\n",
      "2.2937307357788086\n",
      "2.295003652572632\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36467/3359091582.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Use cpu since at this size, GPU with overhead is slower than using CPU directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mssnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSubspaceConstrainedMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubspace_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36467/4180813302.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, num_epochs, train_loader, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;31m# forward pass, calculate loss and backprop!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drift/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drift/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drift/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drift/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drift/lib/python3.9/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drift/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drift/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/drift/lib/python3.9/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for d in dims:\n",
    "    # loss_histories_per_dim = {}\n",
    "    # acc_histories_per_dim = {}\n",
    "    # test_losses_per_dim = {}\n",
    "    corrects_per_dim = {}\n",
    "    for i in range(10):\n",
    "        # Use cpu since at this size, GPU with overhead is slower than using CPU directly\n",
    "        ssnet = SubspaceConstrainedMNIST(subspace_features=d, device=\"cpu\")\n",
    "        loss_history, acc_history = train(ssnet, 15, train_loader, device=\"cpu\")\n",
    "        test_loss, correct = eval(ssnet, test_loader, device=\"cpu\")\n",
    "\n",
    "        # Store everything\n",
    "        # loss_histories_per_dim[i] = loss_history\n",
    "        # acc_histories_per_dim[i] = acc_history\n",
    "        # test_losses_per_dim[i] = test_loss\n",
    "        corrects_per_dim[i] = correct / 10000 * 100\n",
    "        \n",
    "    # loss_histories[d] = loss_histories_per_dim\n",
    "    # acc_histories[d] = acc_histories_per_dim\n",
    "    # test_losses[d] = test_losses_per_dim\n",
    "    corrects[d] = corrects_per_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4569f5f7-08e8-4a0b-ac2f-123aac0c99aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_scores = pd.DataFrame.from_dict(corrects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2bb00f-6779-4499-ae15-d8a4a7f5106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_scores.columns = [str(i) for i in dim_scores.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea565f-1e53-417d-9694-69bfb699937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0b9b48-bbc5-4ced-a0a9-af91e6e6c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_scores.to_csv(\"mnist-scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684e2d70-4c85-43d7-8dfa-f913b9abcb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dim_scores.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5474444-3ea8-4e38-b951-8b8009c7c024",
   "metadata": {},
   "source": [
    "This crashed at some point. I think it is due to the dense matrix mults at large sizes. Putting into script so it is easier to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e29ae2-b0ad-4000-b5a2-28c0494c48c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:drift]",
   "language": "python",
   "name": "conda-env-drift-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
